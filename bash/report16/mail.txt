2020-04-19 23:54 George Jones <address@hidden>:
> It looks like hash_search just does a linear walk if array entries
> to find elements in a list.

https://eludom.github.io/blog/20200418/
> and there it is, the linear search walking the list in hash_search()
>
> ```
> [...]
>
>   bucket = HASH_BUCKET (string, table, hv);
> 
>   for (list = table->bucket_array ? table->bucket_array[bucket] : 0; list; list = list->next)
>     {
> [...]
> ```

The associative arrays in `hashlib.c' are implemented by hash tables
as is clear from its name.  The main lookup of hash table algorithm is
done by the following line

  bucket = HASH_BUCKET (string, table, hv);

but not by the subsequent linear search.  The linear search is just a
workaround for the collision of hashes.  As far as the load factor of
the hash table is maintained properly, the linear search is O(1)
because the length of the list is O(1).

2020-04-19 23:54 George Jones <address@hidden>:
> This slows down (order N?) new inserts when the number of entries
> gets large.

I looked into `hashlib.c' and found that rehashing is actually not
implemented.  I just added a function to perform rehashing, and
the performance has been improved.

2020-04-19 23:54 George Jones <address@hidden>:
> Would there be any interest in merging a patch to add an option for
> making this faster (maybe using b-trees?)

https://eludom.github.io/blog/20200418/
> TODO Look for appropriate in-memory hash insert/lookup functions
> -  btrees ?

The B-tree is not the most appropriate option here.  The hash table is
more appropriate.  The B-tree can be used in the case that we want to
keep the ordering of the keys of associative arrays (e.g. we want to
enumerate items in ascending/descending order).  Bash associative
arrays do not ensure the ordering of the items, so the hash table can
be used as a more efficient choice and is already implemented in
`hashlib.c'.  We can simply add rehashing.

------------------------------------------------------------------------

I attach a patch `0001-hashlib-Implement-rehash.patch' which
implements the rehashing.

I also tested the performance.  The attached `test1.png' shows the
computational time of insertion of items before and after the fix.
The lines are fitted by the function `Time = C Size^alpha' where alpha
is the exponent.  Before the fix, there are two regimes depending on
the number of items: linear regime O(N) (alpha ~ 1) and quadratic
regime O(N^2) (alpha ~ 2).  The quadratic regime signals the linear
scaling of a single insertion.  After the fix, the prformance has been
improved, and the computational time scales linearly in entire region.
I also attach a script `test1.sh' which was used to measure the time.

--
Koichi


------------------------------------------------------------------------


2020-04-20 10:00 George Jones <fooologist@gmail.com>:
> Thank you.  Patch applied and (performance) tested with come tests
> I was working on
> https://github.com/eludom/snippits/tree/master/bash/tests
> .... bottom line:

Thank you for the measurements.

Also, I am sorry that I disturbed your plan for contributing to Bash.
I actually initially doubted that the insertion with the current
implementation is O(N), so I created the test first and then found
that it is an easy fix rather than reimplementing it by B-tree or
other data structures.  I couldn't stop my interest in how much it is
improved by the easy fix.

Nevertheless, I have not tuned the parameters of rehashing.  Actually
it is a tradeoff between the memory consumption and the computational
time, so it is a matter of preference to some extent.  I attach an
updated patch which exposes some parameters.  If you have an interest,
you can play by changing the value of the parameters
`HASH_REHASH_MULTIPLIER' and `HASH_MAX_LOADFACTOR' defined in
`hashlib.h'.

--
Koichi

------------------------------------------------------------------------

Yes, thank you for clarification and sorry for confusing writing.  I
did not mean George's contribution disappeared, but just I have
disturbed his five-step plan on his blog where the remaining four
steps are now canceled.  I have to add that identifying the problem is
definitely the most non-trivial part because it is relatively
straightforward to fix the problem once the problem is identified :).
Also, I would like to thank George for additional testing of my patch.

Of course, this also applies to my patches.  I think I have sent more
than ten patches so far and many of them have been applied, but I am
anyway happy if the problems are solved.  You can always reject my
patches when you have better solutions.

--
Koichi
